{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab | Natural Language Processing\n",
    "### SMS: SPAM or HAM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's prepare the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Read Data for the Fraudulent Email Kaggle Challenge\n",
    "- Reduce the training set to speead up development. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 1)\n"
     ]
    }
   ],
   "source": [
    "## Read Data for the Fraudulent Email Kaggle Challenge\n",
    "data = pd.read_csv(\"/Users/chandlershortlidge/Desktop/Ironhack/lab-natural-language-processing/data/kg_test.csv\",encoding='latin-1')\n",
    "\n",
    "# Reduce the training set to speed up development. \n",
    "# Modify for final system\n",
    "data = data.head(1000)\n",
    "print(data.shape)\n",
    "data.fillna(\"\",inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>usiness is for the fact that the deceased man ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>They are happy to adjust to the afternoon. I a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Lael Brainard was confirmed 78-19 this afterno...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>H &lt;hrod17@clintonemail.com&gt;Friday March 26 201...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>n;\"&gt; Dear Good Friend,&lt;br&gt;&lt;br&gt;&lt;br&gt;I am happy t...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text\n",
       "0  usiness is for the fact that the deceased man ...\n",
       "1  They are happy to adjust to the afternoon. I a...\n",
       "2  Lael Brainard was confirmed 78-19 this afterno...\n",
       "3  H <hrod17@clintonemail.com>Friday March 26 201...\n",
       "4  n;\"> Dear Good Friend,<br><br><br>I am happy t..."
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's divide the training and test set into two partitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set: (1000, 2)\n",
      "Validation set: (1000, 1)\n"
     ]
    }
   ],
   "source": [
    "# Your code\n",
    "\n",
    "# Load training data\n",
    "data_train = pd.read_csv(\"../data/kg_train.csv\", encoding='latin-1')\n",
    "data_train = data_train.head(1000)  # Reduce for faster development\n",
    "data_train.fillna(\"\", inplace=True)\n",
    "\n",
    "# Load test/validation data\n",
    "data_val = pd.read_csv(\"../data/kg_test.csv\", encoding='latin-1')\n",
    "data_val = data_val.head(1000)  # Reduce for faster development\n",
    "data_val.fillna(\"\", inplace=True)\n",
    "\n",
    "print(f\"Training set: {data_train.shape}\")\n",
    "print(f\"Validation set: {data_val.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\n",
      "['needn', \"needn't\", 'no', 'nor', 'not', 'now', 'o', 'of', 'off', 'on']\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "print(string.punctuation)\n",
    "print(stopwords.words(\"english\")[100:110])\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "snowball = SnowballStemmer('english')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now, we have to clean the html code removing words\n",
    "\n",
    "- First we remove inline JavaScript/CSS\n",
    "- Then we remove html comments. This has to be done before removing regular tags since comments can contain '>' characters\n",
    "- Next we can remove the remaining tags"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Remove all the special characters\n",
    "    \n",
    "- Remove numbers\n",
    "    \n",
    "- Remove all single characters\n",
    " \n",
    "- Remove single characters from the start\n",
    "\n",
    "- Substitute multiple spaces with single space\n",
    "\n",
    "- Remove prefixed 'b'\n",
    "\n",
    "- Convert to Lowercase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing complete!\n",
      "                                                text  \\\n",
      "0  DEAR SIR, STRICTLY A PRIVATE BUSINESS PROPOSAL...   \n",
      "1                                           Will do.   \n",
      "\n",
      "                                   preprocessed_text  \n",
      "0  dear sir strictly private business proposal am...  \n",
      "1                                            will do  \n"
     ]
    }
   ],
   "source": [
    "# Your code\n",
    "\n",
    "import re\n",
    "\n",
    "def preprocess_text(text):\n",
    "    # Remove inline JavaScript/CSS\n",
    "    text = re.sub(r'<script[^>]*>.*?</script>', '', text, flags=re.DOTALL)\n",
    "    text = re.sub(r'<style[^>]*>.*?</style>', '', text, flags=re.DOTALL)\n",
    "    \n",
    "    # Remove HTML comments\n",
    "    text = re.sub(r'<!--.*?-->', '', text, flags=re.DOTALL)\n",
    "    \n",
    "    # Remove HTML tags\n",
    "    text = re.sub(r'<[^>]+>', '', text)\n",
    "    \n",
    "    # Remove special characters (keep only letters, numbers, spaces)\n",
    "    text = re.sub(r'[^a-zA-Z0-9\\s]', '', text)\n",
    "    \n",
    "    # Remove numbers\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "    \n",
    "    # Remove all single characters (standalone letters)\n",
    "    text = re.sub(r'\\b[a-zA-Z]\\b', '', text)\n",
    "    \n",
    "    # Remove single characters from the start\n",
    "    text = re.sub(r'^[a-zA-Z]\\s', '', text)\n",
    "    \n",
    "    # Substitute multiple spaces with single space\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    \n",
    "    # Remove prefixed 'b' (like b'string' from bytes)\n",
    "    text = re.sub(r\"^b'\", '', text)\n",
    "    \n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    return text\n",
    "\n",
    "# Create preprocessed_text column by applying to original text\n",
    "data_train['preprocessed_text'] = data_train['text'].apply(preprocess_text)\n",
    "data_val['preprocessed_text'] = data_val['text'].apply(preprocess_text)\n",
    "\n",
    "print(\"Preprocessing complete!\")\n",
    "print(data_train[['text', 'preprocessed_text']].head(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "## Now let's work on removing stopwords\n",
    "Remove the stopwords."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    words = text.split()\n",
    "    filtered_words = [word for word in words if word.lower() not in stop_words]\n",
    "    return ' '.join(filtered_words)\n",
    "\n",
    "# Apply to your data\n",
    "data_train['preprocessed_text'] = data_train['preprocessed_text'].apply(remove_stopwords)\n",
    "data_val['preprocessed_text'] = data_val['preprocessed_text'].apply(remove_stopwords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tame Your Text with Lemmatization\n",
    "Break sentences into words, then use lemmatization to reduce them to their base form (e.g., \"running\" becomes \"run\"). See how this creates cleaner data for analysis!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/chandlershortlidge/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Your code\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def lemmatize_text(text):\n",
    "    words = text.split()\n",
    "    lemmatized = [lemmatizer.lemmatize(word) for word in words]\n",
    "    return ' '.join(lemmatized)\n",
    "\n",
    "# Apply to your data\n",
    "data_train['preprocessed_text'] = data_train['preprocessed_text'].apply(lemmatize_text)\n",
    "data_val['preprocessed_text'] = data_val['preprocessed_text'].apply(lemmatize_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bag Of Words\n",
    "Let's get the 10 top words in ham and spam messages (**EXPLORATORY DATA ANALYSIS**)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 ham words:\n",
      "[('u', 115), ('pm', 115), ('would', 106), ('state', 103), ('president', 94), ('call', 91), ('time', 84), ('percent', 77), ('secretary', 76), ('work', 73)]\n",
      "\n",
      "Top 10 spam words:\n",
      "[('money', 920), ('account', 794), ('bank', 745), ('fund', 703), ('u', 550), ('business', 473), ('transaction', 416), ('country', 406), ('transfer', 392), ('million', 385)]\n"
     ]
    }
   ],
   "source": [
    "# Your code\n",
    "from collections import Counter\n",
    "\n",
    "# Split by label\n",
    "ham_texts = data_train[data_train['label'] == 0]['preprocessed_text']\n",
    "spam_texts = data_train[data_train['label'] == 1]['preprocessed_text']\n",
    "\n",
    "# Combine all words for each\n",
    "ham_words = ' '.join(ham_texts).split()\n",
    "spam_words = ' '.join(spam_texts).split()\n",
    "\n",
    "# Count and get top 10\n",
    "print(\"Top 10 ham words:\")\n",
    "print(Counter(ham_words).most_common(10))\n",
    "\n",
    "print(\"\\nTop 10 spam words:\")\n",
    "print(Counter(spam_words).most_common(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extra features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>preprocessed_text</th>\n",
       "      <th>money_mark</th>\n",
       "      <th>suspicious_words</th>\n",
       "      <th>text_len</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>DEAR SIR, STRICTLY A PRIVATE BUSINESS PROPOSAL...</td>\n",
       "      <td>1</td>\n",
       "      <td>dear sir strictly private business proposal mi...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1504</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Will do.</td>\n",
       "      <td>0</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Nora--Cheryl has emailed dozens of memos about...</td>\n",
       "      <td>0</td>\n",
       "      <td>noracheryl emailed dozen memo haiti weekend pl...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Dear Sir=2FMadam=2C I know that this proposal ...</td>\n",
       "      <td>1</td>\n",
       "      <td>dear sirfmadamc know proposal might surprise e...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1382</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>fyi</td>\n",
       "      <td>0</td>\n",
       "      <td>fyi</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  label  \\\n",
       "0  DEAR SIR, STRICTLY A PRIVATE BUSINESS PROPOSAL...      1   \n",
       "1                                           Will do.      0   \n",
       "2  Nora--Cheryl has emailed dozens of memos about...      0   \n",
       "3  Dear Sir=2FMadam=2C I know that this proposal ...      1   \n",
       "4                                                fyi      0   \n",
       "\n",
       "                                   preprocessed_text  money_mark  \\\n",
       "0  dear sir strictly private business proposal mi...           1   \n",
       "1                                                              0   \n",
       "2  noracheryl emailed dozen memo haiti weekend pl...           0   \n",
       "3  dear sirfmadamc know proposal might surprise e...           1   \n",
       "4                                                fyi           0   \n",
       "\n",
       "   suspicious_words  text_len  \n",
       "0                 1      1504  \n",
       "1                 0         0  \n",
       "2                 0       110  \n",
       "3                 1      1382  \n",
       "4                 0         3  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We add to the original dataframe two additional indicators (money symbols and suspicious words).\n",
    "money_simbol_list = \"|\".join([\"euro\",\"dollar\",\"pound\",\"€\",r\"\\$\"])\n",
    "suspicious_words = \"|\".join([\"free\",\"cheap\",\"sex\",\"money\",\"account\",\"bank\",\"fund\",\"transfer\",\"transaction\",\"win\",\"deposit\",\"password\"])\n",
    "\n",
    "data_train['money_mark'] = data_train['preprocessed_text'].str.contains(money_simbol_list)*1\n",
    "data_train['suspicious_words'] = data_train['preprocessed_text'].str.contains(suspicious_words)*1\n",
    "data_train['text_len'] = data_train['preprocessed_text'].apply(lambda x: len(x)) \n",
    "\n",
    "data_val['money_mark'] = data_val['preprocessed_text'].str.contains(money_simbol_list)*1\n",
    "data_val['suspicious_words'] = data_val['preprocessed_text'].str.contains(suspicious_words)*1\n",
    "data_val['text_len'] = data_val['preprocessed_text'].apply(lambda x: len(x)) \n",
    "\n",
    "data_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How would work the Bag of Words with Count Vectorizer concept?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 19352\n",
      "Training BoW shape: (1000, 19352)\n",
      "Validation BoW shape: (1000, 19352)\n",
      "\n",
      "First 10 words in vocabulary: ['aac' 'aaclocated' 'aae' 'aag' 'aaronovitchon' 'abacha' 'abachabefore'\n",
      " 'abachac' 'abachace' 'abachaco']\n"
     ]
    }
   ],
   "source": [
    "# Your code\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Create the vectorizer\n",
    "count_vectorizer = CountVectorizer()\n",
    "\n",
    "# Fit and transform the training data\n",
    "bow_train = count_vectorizer.fit_transform(data_train['preprocessed_text'])\n",
    "\n",
    "# Transform the validation data (using the same vocabulary)\n",
    "bow_val = count_vectorizer.transform(data_val['preprocessed_text'])\n",
    "\n",
    "print(f\"Vocabulary size: {len(count_vectorizer.vocabulary_)}\")\n",
    "print(f\"Training BoW shape: {bow_train.shape}\")\n",
    "print(f\"Validation BoW shape: {bow_val.shape}\")\n",
    "\n",
    "# Show example: first 10 words in vocabulary\n",
    "feature_names = count_vectorizer.get_feature_names_out()\n",
    "print(f\"\\nFirst 10 words in vocabulary: {feature_names[:10]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF\n",
    "\n",
    "- Load the vectorizer\n",
    "\n",
    "- Vectorize all dataset\n",
    "\n",
    "- print the shape of the vetorized dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape: (1000, 19352)\n",
      "Val shape: (1000, 19352)\n"
     ]
    }
   ],
   "source": [
    "# Your code\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "X_train = vectorizer.fit_transform(data_train[\"preprocessed_text\"])\n",
    "X_val = vectorizer.transform(data_val[\"preprocessed_text\"])\n",
    "\n",
    "# Print shape\n",
    "print(\"Train shape:\", X_train.shape)\n",
    "print(\"Val shape:\", X_val.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## And the Train a Classifier?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      1.00      0.95       125\n",
      "           1       1.00      0.84      0.91        75\n",
      "\n",
      "    accuracy                           0.94       200\n",
      "   macro avg       0.96      0.92      0.93       200\n",
      "weighted avg       0.95      0.94      0.94       200\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split training data into train and validation\n",
    "X_train_split, X_val_split, y_train, y_val = train_test_split(\n",
    "    X_train, data_train['label'], test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Train\n",
    "model = LogisticRegression(max_iter=1000)\n",
    "model.fit(X_train_split, y_train)\n",
    "\n",
    "# Predict\n",
    "y_pred = model.predict(X_val_split)\n",
    "\n",
    "# Evaluate\n",
    "print(classification_report(y_val, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extra Task - Implement a SPAM/HAM classifier\n",
    "\n",
    "https://www.kaggle.com/t/b384e34013d54d238490103bc3c360ce\n",
    "\n",
    "The classifier can not be changed!!! It must be the MultinimialNB with default parameters!\n",
    "\n",
    "Your task is to **find the most relevant features**.\n",
    "\n",
    "For example, you can test the following options and check which of them performs better:\n",
    "- Using \"Bag of Words\" only\n",
    "- Using \"TF-IDF\" only\n",
    "- Bag of Words + extra flags (money_mark, suspicious_words, text_len)\n",
    "- TF-IDF + extra flags\n",
    "\n",
    "\n",
    "You can work with teams of two persons (recommended)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.93      0.94       125\n",
      "           1       0.89      0.93      0.91        75\n",
      "\n",
      "    accuracy                           0.93       200\n",
      "   macro avg       0.92      0.93      0.93       200\n",
      "weighted avg       0.93      0.93      0.93       200\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Your code\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "X_train = vectorizer.fit_transform(data_train['preprocessed_text'])\n",
    "\n",
    "# Split training data into train and validation\n",
    "X_train_split, X_val_split, y_train, y_val = train_test_split(\n",
    "    X_train, data_train['label'], test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Train\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "model = MultinomialNB()\n",
    "model.fit(X_train_split, y_train)\n",
    "\n",
    "# Predict\n",
    "y_pred = model.predict(X_val_split)\n",
    "\n",
    "# Evaluate\n",
    "print(classification_report(y_val, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.92      0.95       125\n",
      "           1       0.88      0.97      0.92        75\n",
      "\n",
      "    accuracy                           0.94       200\n",
      "   macro avg       0.93      0.95      0.94       200\n",
      "weighted avg       0.94      0.94      0.94       200\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "X_train = vectorizer.fit_transform(data_train['preprocessed_text'])\n",
    "\n",
    "# Split training data into train and validation\n",
    "X_train_split, X_val_split, y_train, y_val = train_test_split(\n",
    "    X_train, data_train['label'], test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Train\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "model = MultinomialNB()\n",
    "model.fit(X_train_split, y_train)\n",
    "\n",
    "# Predict\n",
    "y_pred = model.predict(X_val_split)\n",
    "\n",
    "# Evaluate\n",
    "print(classification_report(y_val, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.89      0.93       125\n",
      "           1       0.84      0.96      0.89        75\n",
      "\n",
      "    accuracy                           0.92       200\n",
      "   macro avg       0.91      0.92      0.91       200\n",
      "weighted avg       0.92      0.92      0.92       200\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from scipy.sparse import hstack\n",
    "import numpy as np\n",
    "\n",
    "# Create extra features (do this BEFORE the train/test split)\n",
    "data_train['money_mark'] = data_train['text'].str.contains('\\$|£|€|money|cash', regex=True).astype(int)\n",
    "data_train['suspicious_words'] = data_train['text'].str.lower().str.contains('free|win|click|urgent|claim', regex=True).astype(int)\n",
    "\n",
    "# Vectorize text\n",
    "vectorizer = TfidfVectorizer()\n",
    "X_text = vectorizer.fit_transform(data_train['preprocessed_text'])\n",
    "\n",
    "# Get extra features as array\n",
    "extra_features = data_train[['money_mark', 'suspicious_words']].values\n",
    "\n",
    "# Combine them\n",
    "X_combined = hstack([X_text, extra_features])\n",
    "\n",
    "# Now do your train/test split on X_combined\n",
    "X_train_split, X_val_split, y_train, y_val = train_test_split(\n",
    "    X_combined, data_train['label'], test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Train and evaluate\n",
    "model = MultinomialNB()\n",
    "model.fit(X_train_split, y_train)\n",
    "y_pred = model.predict(X_val_split)\n",
    "print(classification_report(y_val, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
